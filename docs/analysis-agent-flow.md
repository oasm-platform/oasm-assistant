# Luá»“ng Hoáº¡t Äá»™ng cá»§a Analysis Agent

## Tá»•ng Quan

Analysis Agent lÃ  má»™t agent chuyÃªn biá»‡t trong há»‡ thá»‘ng OASM Assistant, cÃ³ nhiá»‡m vá»¥ phÃ¢n tÃ­ch dá»¯ liá»‡u báº£o máº­t tá»« workspace thÃ´ng qua MCP (Model Context Protocol) vÃ  tráº£ vá» káº¿t quáº£ phÃ¢n tÃ­ch vá»›i **LLM streaming real-time** (nhÆ° ChatGPT/Claude).

## Kiáº¿n TrÃºc Tá»•ng Thá»ƒ

```
User Input (gRPC Streaming)
    â†“
MessageService.CreateMessage() [Streaming]
    â†“
SecurityCoordinator.process_message_question_streaming() [Async]
    â†“
LangGraph Workflow (Router)
    â†“
AnalysisAgent.execute_task_streaming() [Async]
    â†“
MCP Tools (Dynamic Discovery)
    â†“
LLM.astream() [Real-time Streaming]
    â†“
Stream to User (Buffered Chunks)
```

---

## CÃ¡c Thay Äá»•i Quan Trá»ng

### ğŸ†• **Streaming Architecture**
- **LLM Streaming**: Text Ä‘Æ°á»£c stream tá»« LLM nhÆ° ChatGPT/Claude
- **Async/Await**: ToÃ n bá»™ pipeline sá»­ dá»¥ng async generators
- **Buffered Chunks**: Gom nhá» chunks Ä‘á»ƒ giáº£m sá»‘ responses (configurable)
- **No Manual Chunking**: KhÃ´ng cÃ²n DELTA_CHUNK_SIZE thá»§ cÃ´ng

### ğŸ”§ **Cáº¥u hÃ¬nh Streaming**
```bash
# config/.env
LLM_MIN_CHUNK_SIZE=20  # Minimum chars before sending (default: 20)
```

---

## Chi Tiáº¿t Luá»“ng Hoáº¡t Äá»™ng

### 1. Entry Point: MessageService (app/services/message.py)

**File:** `app/services/message.py`
**Method:** `CreateMessage()` - gRPC Streaming

#### Luá»“ng Streaming:

```python
def CreateMessage(self, request, context):
    # Step 1: Setup
    question = request.question.strip()
    workspace_id = context.workspace_id
    user_id = context.user_id
    message_id = str(uuid.uuid4())

    # Step 2: Initialize coordinator
    coordinator = SecurityCoordinator(
        db_session=session,
        workspace_id=workspace_id,
        user_id=user_id
    )

    # Step 3: Create async streaming generator
    streaming_events = coordinator.process_message_question_streaming(question)

    # Step 4: Build async response stream
    async_stream = StreamingResponseBuilder.build_response_stream(
        message_id=message_id,
        conversation_id=conversation_id,
        question=question,
        response_generator=streaming_events
    )

    # Step 5: Convert async â†’ sync for gRPC compatibility
    for stream_message in async_generator_to_sync(async_stream):
        # Accumulate delta text for DB storage
        if stream_message.type == "delta":
            accumulated_answer.append(stream_message.text)

        # Yield streaming message to client
        yield assistant_pb2.CreateMessageResponse(message=stream_message)

    # Step 6: Save to database after streaming completes
    answer = "".join(accumulated_answer)
    save_to_database(question, answer)
```

#### Event Types Ä‘Æ°á»£c Stream:

| Event Type | Description | Example |
|------------|-------------|---------|
| `message_start` | Báº¯t Ä‘áº§u message | `{"message_id": "xxx", "timestamp": "..."}` |
| `thinking` | Agent Ä‘ang suy nghÄ© | `{"thought": "Analyzing security data..."}` |
| `tool_start` | Báº¯t Ä‘áº§u dÃ¹ng tool | `{"tool_name": "mcp_data_fetch"}` |
| `tool_output` | Káº¿t quáº£ tool | `{"status": "success", "output": {...}}` |
| `tool_end` | HoÃ n thÃ nh tool | `{"summary": "Completed data fetch"}` |
| `delta` â­ | **LLM text chunk** | `{"text": "Based on the scan data..."}` |
| `message_end` | Káº¿t thÃºc message | `{"total_time_ms": 5000, "success": true}` |
| `done` | HoÃ n táº¥t | `{"final_status": "success"}` |

---

### 2. SecurityCoordinator (agents/workflows/security_coordinator.py)

**File:** `agents/workflows/security_coordinator.py`
**Class:** `SecurityCoordinator`

#### 2.1. Process Message Question Streaming

```python
async def process_message_question_streaming(
    self,
    question: str
) -> AsyncGenerator[Dict[str, Any], None]:
    """
    Process question with real-time streaming

    Yields:
        Streaming events from agents
    """
    try:
        # Yield thinking event
        yield {
            "type": "thinking",
            "agent": "SecurityCoordinator",
            "thought": "Analyzing security question and determining workflow",
            "roadmap": [
                {"step": "1", "description": "Route to appropriate security agent"},
                {"step": "2", "description": "Execute security analysis with LLM"},
                {"step": "3", "description": "Stream response to user"}
            ]
        }

        # Create agent
        agent = AnalysisAgent(
            db_session=self.db_session,
            workspace_id=self.workspace_id,
            user_id=self.user_id
        )

        # Prepare task
        task = {
            "action": "analyze_vulnerabilities",
            "question": question
        }

        # Stream agent execution (async for)
        async for event in agent.execute_task_streaming(task):
            yield event

    except Exception as e:
        yield {
            "type": "error",
            "error_message": str(e),
            "agent": "SecurityCoordinator"
        }
```

**Key Points:**
- âœ… **Async Generator**: Sá»­ dá»¥ng `async def` vá»›i `AsyncGenerator` return type
- âœ… **Direct Streaming**: KhÃ´ng qua LangGraph cho Ä‘Æ¡n giáº£n
- âœ… **Error Handling**: Yield error events thay vÃ¬ raise exceptions

---

### 3. AnalysisAgent (agents/specialized/analysis_agent.py)

**File:** `agents/specialized/analysis_agent.py`
**Class:** `AnalysisAgent`

#### 3.1. Execute Task Streaming

```python
async def execute_task_streaming(
    self,
    task: Dict[str, Any]
) -> AsyncGenerator[Dict[str, Any], None]:
    """
    Execute task with streaming support

    Yields streaming events like ChatGPT/Claude
    """
    try:
        action = task.get("action", "analyze_vulnerabilities")
        question = task.get("question")

        # Yield thinking event
        yield {
            "type": "thinking",
            "thought": "Analyzing security data and preparing response",
            "agent": self.name
        }

        if action == "analyze_vulnerabilities":
            async for event in self.analyze_vulnerabilities_streaming(question):
                yield event
        else:
            yield {
                "type": "error",
                "error": f"Unknown action: {action}",
                "agent": self.name
            }

    except Exception as e:
        yield {
            "type": "error",
            "error": str(e),
            "agent": self.name
        }
```

#### 3.2. Analyze Vulnerabilities Streaming

```python
async def analyze_vulnerabilities_streaming(
    self,
    question: str
) -> AsyncGenerator[Dict[str, Any], None]:
    """
    Streaming vulnerability analysis - like ChatGPT/Claude

    Yields:
        Streaming events with LLM text chunks as they are generated
    """
    # Step 1: Yield tool start - fetching MCP data
    yield {
        "type": "tool_start",
        "tool_name": "mcp_data_fetch",
        "tool_description": "Fetching security scan data from MCP",
        "agent": self.name
    }

    # Step 2: Fetch MCP data
    scan_data = await self._fetch_mcp_data(question)

    # Step 3: Yield tool output
    yield {
        "type": "tool_output",
        "tool_name": "mcp_data_fetch",
        "status": "success" if scan_data else "no_data",
        "output": {
            "has_data": bool(scan_data),
            "source": scan_data.get("source") if scan_data else None
        },
        "agent": self.name
    }

    # Step 4: Yield tool_end
    yield {
        "type": "tool_end",
        "tool_name": "mcp_data_fetch",
        "agent": self.name
    }

    # Step 5: Stream LLM analysis (REAL-TIME STREAMING)
    async for event in self._generate_analysis_streaming(question, scan_data):
        yield event

    # Step 6: Yield final result
    yield {
        "type": "result",
        "data": {
            "success": bool(scan_data),
            "has_data": bool(scan_data)
        },
        "agent": self.name
    }
```

#### 3.3. Generate Analysis Streaming (â­ Key Method)

```python
async def _generate_analysis_streaming(
    self,
    question: str,
    scan_data: Optional[Dict]
) -> AsyncGenerator[Dict[str, Any], None]:
    """
    Generate analysis response with LLM streaming (like ChatGPT/Claude)

    Yields delta events as LLM generates text (buffered to reduce responses)
    """
    # Get min chunk size from config
    min_chunk_size = configs.llm.min_chunk_size  # Default: 20

    # Select appropriate prompt
    if not scan_data:
        prompt = AnalysisAgentPrompts.get_no_data_response_prompt(question)
    else:
        stats = scan_data.get("stats", {})
        tool = scan_data.get("tool", "unknown")

        if "statistics" in tool or "score" in str(stats):
            prompt = AnalysisAgentPrompts.get_statistics_analysis_prompt(question, stats)
        elif "vulnerabilities" in tool:
            prompt = AnalysisAgentPrompts.get_vulnerabilities_analysis_prompt(question, stats)
        elif "assets" in tool:
            prompt = AnalysisAgentPrompts.get_assets_analysis_prompt(question, stats)
        else:
            prompt = AnalysisAgentPrompts.get_generic_analysis_prompt(question, stats)

    # â­ Stream LLM response with buffering
    async for buffered_text in self._buffer_llm_chunks(
        self.llm.astream(prompt),
        min_chunk_size
    ):
        yield {
            "type": "delta",
            "text": buffered_text,
            "agent": self.name
        }
```

#### 3.4. Buffer LLM Chunks (â­ Performance Optimization)

```python
async def _buffer_llm_chunks(
    self,
    llm_stream: AsyncGenerator,
    min_chunk_size: int
) -> AsyncGenerator[str, None]:
    """
    Buffer LLM chunks to reduce number of responses sent

    Args:
        llm_stream: Async generator from LLM (self.llm.astream())
        min_chunk_size: Minimum characters before yielding (from config)

    Yields:
        Buffered text chunks

    Example:
        LLM generates: "Based" â†’ Buffer (5 chars)
        LLM generates: " on" â†’ Buffer (8 chars)
        LLM generates: " the" â†’ Buffer (12 chars)
        LLM generates: " scan" â†’ Buffer (17 chars)
        LLM generates: " data" â†’ Buffer (22 chars) â†’ YIELD "Based on the scan data" âœ…
    """
    buffer = ""

    async for chunk in llm_stream:
        # Extract text from chunk
        if isinstance(chunk, BaseMessage) and chunk.content:
            text = chunk.content
        elif isinstance(chunk, str):
            text = chunk
        else:
            continue

        buffer += text

        # Yield when buffer reaches min size
        if len(buffer) >= min_chunk_size:
            yield buffer
            buffer = ""

    # Yield remaining buffer
    if buffer:
        yield buffer
```

**Benefits of Buffering:**
- âœ… Reduces number of gRPC responses (less overhead)
- âœ… More natural text flow for users
- âœ… Configurable via `LLM_MIN_CHUNK_SIZE` env var
- âœ… Still maintains real-time streaming feel

---

### 4. Streaming Response Builder (app/services/streaming_handler.py)

**File:** `app/services/streaming_handler.py`
**Class:** `StreamingResponseBuilder`

#### Build Response Stream

```python
@staticmethod
async def build_response_stream(
    message_id: str,
    conversation_id: str,
    question: str,
    response_generator: AsyncGenerator[Dict[str, Any], None]
) -> AsyncGenerator[assistant_pb2.Message, None]:
    """
    Build a complete streaming response from an async generator

    Converts agent events to gRPC Message protobuf objects
    """
    handler = StreamingMessageHandler(message_id, conversation_id, question)

    try:
        # Send message_start
        yield handler.message_start()

        # Process events from async generator
        async for event in response_generator:
            event_type = event.get("type")

            if event_type == "thinking":
                yield handler.thinking(
                    agent=event.get("agent", ""),
                    thought=event.get("thought", ""),
                    roadmap=event.get("roadmap"),
                    context=event.get("context")
                )

            elif event_type == "tool_start":
                yield handler.tool_start(
                    tool_name=event.get("tool_name", ""),
                    tool_description=event.get("tool_description", ""),
                    parameters=event.get("parameters", {}),
                    agent=event.get("agent", "")
                )

            elif event_type == "tool_output":
                yield handler.tool_output(
                    tool_name=event.get("tool_name", ""),
                    status=event.get("status", "success"),
                    agent=event.get("agent", ""),
                    output=event.get("output")
                )

            elif event_type == "tool_end":
                yield handler.tool_end(
                    tool_name=event.get("tool_name", ""),
                    agent=event.get("agent", ""),
                    summary=event.get("summary", "")
                )

            elif event_type == "delta":  # â­ LLM text streaming
                yield handler.delta(
                    text=event.get("text", ""),
                    agent=event.get("agent", "")
                )

            elif event_type == "error":
                yield handler.error(
                    error_type=event.get("error_type", ""),
                    error_message=event.get("error_message", ""),
                    agent=event.get("agent", "")
                )

        # Send message_end
        yield handler.message_end(success=True)

        # Send done
        yield handler.done(final_status="success")

    except Exception as e:
        logger.error(f"Error in streaming response: {e}", exc_info=True)
        yield handler.error(
            error_type="StreamingError",
            error_message=str(e),
            agent="StreamingResponseBuilder"
        )
        yield handler.done(final_status="error")
```

---

### 5. Async to Sync Conversion (app/services/message.py)

**Helper Function:** `async_generator_to_sync()`

```python
def async_generator_to_sync(async_gen):
    """
    Convert async generator to sync generator for gRPC compatibility

    Why needed:
    - gRPC servicer methods must be sync generators
    - Our agents use async generators for streaming
    - This bridges the gap

    Args:
        async_gen: Async generator to convert

    Yields:
        Items from the async generator (synchronously)
    """
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        while True:
            try:
                # Run async __anext__() in event loop
                yield loop.run_until_complete(async_gen.__anext__())
            except StopAsyncIteration:
                break
    finally:
        loop.close()
```

**Why Needed:**
- gRPC servicer methods must return **sync generators** (not async)
- Our agent pipeline uses **async generators** for performance
- This helper bridges the gap between async and sync worlds

---

## SÆ¡ Äá»“ Streaming Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. USER REQUEST (gRPC)                                          â”‚
â”‚    - Question: "Workspace cÃ³ lá»— há»•ng gÃ¬ nghiÃªm trá»ng?"          â”‚
â”‚    - workspace_id, user_id                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. MESSAGE SERVICE                                              â”‚
â”‚    CreateMessage() - gRPC Streaming Generator                   â”‚
â”‚                                                                  â”‚
â”‚    for msg in async_generator_to_sync(async_stream):            â”‚
â”‚        yield CreateMessageResponse(message=msg)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. STREAMING RESPONSE BUILDER (Async)                          â”‚
â”‚    async for event in response_generator:                       â”‚
â”‚        - Convert events to protobuf Messages                    â”‚
â”‚        - Track agents used, tools used                          â”‚
â”‚        - Accumulate past actions                                â”‚
â”‚        yield Message(type=event_type, content=json)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. SECURITY COORDINATOR (Async)                                â”‚
â”‚    async def process_message_question_streaming():              â”‚
â”‚        yield {"type": "thinking", ...}                          â”‚
â”‚        async for event in agent.execute_task_streaming():       â”‚
â”‚            yield event                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. ANALYSIS AGENT (Async)                                      â”‚
â”‚    async def execute_task_streaming():                          â”‚
â”‚        yield {"type": "tool_start", ...}                        â”‚
â”‚        scan_data = await _fetch_mcp_data()                      â”‚
â”‚        yield {"type": "tool_output", ...}                       â”‚
â”‚                                                                  â”‚
â”‚        # â­ Stream LLM response                                  â”‚
â”‚        async for event in _generate_analysis_streaming():       â”‚
â”‚            yield event  # {"type": "delta", "text": "..."}      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. GENERATE ANALYSIS STREAMING                                 â”‚
â”‚    async def _generate_analysis_streaming():                    â”‚
â”‚                                                                  â”‚
â”‚        # Buffer LLM chunks                                      â”‚
â”‚        async for buffered_text in _buffer_llm_chunks(           â”‚
â”‚            self.llm.astream(prompt),                            â”‚
â”‚            min_chunk_size=20                                    â”‚
â”‚        ):                                                        â”‚
â”‚            yield {"type": "delta", "text": buffered_text}       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. BUFFER LLM CHUNKS                                            â”‚
â”‚    async def _buffer_llm_chunks():                              â”‚
â”‚        buffer = ""                                              â”‚
â”‚        async for chunk in self.llm.astream(prompt):             â”‚
â”‚            # Extract text from chunk                            â”‚
â”‚            text = chunk.content if BaseMessage else chunk       â”‚
â”‚            buffer += text                                       â”‚
â”‚                                                                  â”‚
â”‚            # Yield when buffer reaches min size                 â”‚
â”‚            if len(buffer) >= min_chunk_size:                    â”‚
â”‚                yield buffer                                     â”‚
â”‚                buffer = ""                                      â”‚
â”‚                                                                  â”‚
â”‚        # Yield remaining                                        â”‚
â”‚        if buffer:                                               â”‚
â”‚            yield buffer                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. LLM (Ollama/OpenAI/Anthropic)                               â”‚
â”‚    async for chunk in llm.astream(prompt):                      â”‚
â”‚        # Generate text token by token                           â”‚
â”‚        yield "Based"                                            â”‚
â”‚        yield " on"                                              â”‚
â”‚        yield " the"                                             â”‚
â”‚        yield " scan"                                            â”‚
â”‚        yield " data"                                            â”‚
â”‚        ...                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 9. BACK TO USER (Streaming)                                    â”‚
â”‚                                                                  â”‚
â”‚    Stream 1: {"type": "message_start", ...}                     â”‚
â”‚    Stream 2: {"type": "thinking", "thought": "Analyzing..."}    â”‚
â”‚    Stream 3: {"type": "tool_start", "tool_name": "mcp_fetch"}  â”‚
â”‚    Stream 4: {"type": "tool_output", "status": "success"}      â”‚
â”‚    Stream 5: {"type": "tool_end", ...}                         â”‚
â”‚    Stream 6: {"type": "delta", "text": "Based on the scan "}   â”‚
â”‚    Stream 7: {"type": "delta", "text": "data, your workspace"} â”‚
â”‚    Stream 8: {"type": "delta", "text": " has 45 vulnerabilit"} â”‚
â”‚    Stream 9: {"type": "delta", "text": "ies: 3 Critical, "}    â”‚
â”‚    ...                                                           â”‚
â”‚    Stream N: {"type": "message_end", "success": true}          â”‚
â”‚    Stream N+1: {"type": "done", "final_status": "success"}     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Configuration

### Environment Variables (.env)

```bash
# LLM Configuration
LLM_PROVIDER=ollama
LLM_BASE_URL=http://localhost:11434
LLM_MODEL_NAME=llama3
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=4000

# â­ Streaming Configuration
LLM_MIN_CHUNK_SIZE=20  # Min chars before sending chunk (default: 20)
# Lower (5-10): More responsive, more frequent updates
# Higher (30-50): Fewer updates, more text per chunk

# Database
POSTGRES_HOST=localhost
POSTGRES_PORT=9432
POSTGRES_DB=oasm-assistant
```

---

## Performance Considerations

### 1. Buffering Strategy

**Without Buffering (BAD âŒ):**
```
LLM: "B" â†’ Send to user
LLM: "a" â†’ Send to user
LLM: "s" â†’ Send to user
LLM: "e" â†’ Send to user
LLM: "d" â†’ Send to user
â†’ 5 responses for 5 characters! Too many!
```

**With Buffering (GOOD âœ…):**
```
LLM: "B" â†’ Buffer (1 char)
LLM: "a" â†’ Buffer (2 chars)
LLM: "s" â†’ Buffer (3 chars)
LLM: "e" â†’ Buffer (4 chars)
LLM: "d" â†’ Buffer (5 chars)
...
LLM: " " â†’ Buffer (20 chars) â†’ SEND "Based on the scan d"
â†’ 1 response for 20 characters! Much better!
```

### 2. Async/Await Benefits

- **Non-blocking I/O**: KhÃ´ng block khi chá» LLM response
- **Concurrent Processing**: Multiple users Ä‘Æ°á»£c serve Ä‘á»“ng thá»i
- **Memory Efficient**: Generators khÃ´ng load toÃ n bá»™ response vÃ o memory
- **Scalable**: Dá»… scale vá»›i nhiá»u concurrent connections

### 3. gRPC Streaming Benefits

- **Low Latency**: User tháº¥y text ngay khi LLM generate
- **Better UX**: Giá»‘ng ChatGPT/Claude, khÃ´ng pháº£i chá» full response
- **Progressive Rendering**: Frontend cÃ³ thá»ƒ render tá»«ng pháº§n
- **Cancellable**: User cÃ³ thá»ƒ cancel request giá»¯a chá»«ng

---

## Error Handling

### 1. Async Generator Errors

```python
async def execute_task_streaming(self, task):
    try:
        # Normal flow
        async for event in self.analyze_vulnerabilities_streaming(question):
            yield event
    except Exception as e:
        # Yield error event instead of raising
        yield {
            "type": "error",
            "error_message": str(e),
            "agent": self.name,
            "recoverable": False
        }
```

### 2. LLM Streaming Errors

```python
async def _generate_analysis_streaming(self, question, scan_data):
    try:
        async for chunk in self.llm.astream(prompt):
            # Process chunk
            yield {"type": "delta", "text": chunk.content}
    except Exception as e:
        logger.error(f"LLM streaming failed: {e}")
        # Send fallback response
        yield {
            "type": "delta",
            "text": "I apologize, but I encountered an error generating the response."
        }
```

### 3. MCP Errors

```python
async def _fetch_mcp_data(self, question):
    try:
        await self.mcp_manager.initialize()
        result = await self.mcp_manager.call_tool(server, tool, args)
        return result
    except Exception as e:
        logger.error(f"MCP fetch error: {e}")
        return None  # Will trigger "no data" flow
```

---

## Files & Locations

```
oasm-assistant/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ message.py                    # Entry point, async_generator_to_sync()
â”‚       â””â”€â”€ streaming_handler.py          # StreamingResponseBuilder (async)
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ base_agent.py                # BaseAgent with streaming support
â”‚   â”‚
â”‚   â”œâ”€â”€ specialized/
â”‚   â”‚   â””â”€â”€ analysis_agent.py            # AnalysisAgent (async streaming)
â”‚   â”‚
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ security_coordinator.py       # SecurityCoordinator (async streaming)
â”‚
â”œâ”€â”€ common/
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ configs.py                    # LlmConfigs.min_chunk_size
â”‚
â””â”€â”€ llms/
    â”œâ”€â”€ llm_manager.py                    # LLM provider management
    â””â”€â”€ prompts/
        â””â”€â”€ analysis_agent_prompts.py     # Prompt templates
```

---

## Káº¿t Luáº­n

Analysis Agent hiá»‡n táº¡i lÃ  má»™t há»‡ thá»‘ng **streaming-first** vá»›i:

### âœ… **Streaming Architecture**
- LLM streaming real-time (nhÆ° ChatGPT/Claude)
- Async/await throughout
- Buffered chunks Ä‘á»ƒ tá»‘i Æ°u performance

### âœ… **Performance Optimized**
- Configurable `LLM_MIN_CHUNK_SIZE`
- Non-blocking I/O
- Memory efficient generators

### âœ… **User Experience**
- Text xuáº¥t hiá»‡n ngay khi LLM generate
- Smooth, natural flow
- Progressive rendering support

### âœ… **Developer Friendly**
- Clear separation: async agents, sync gRPC
- Easy to debug with event-based architecture
- Configurable for different use cases

**Total Latency Breakdown:**
- Time to first token: ~100-500ms (LLM dependent)
- Streaming overhead: ~5-20ms per chunk (buffered)
- Total response time: Same as before, but user sees results immediately!