services:
  # PostgreSQL with pgvector extension
  oasm-assistant-postgresql:
    image: pgvector/pgvector:pg16
    container_name: oasm-assistant-postgresql
    restart: unless-stopped
    env_file:
      - .env
    ports:
      - "9432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./data/database/extensions:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - oasm-network
    environment:
      - POSTGRES_INITDB_ARGS=--encoding=UTF-8 --lc-collate=en_US.UTF-8 --lc-ctype=en_US.UTF-8

  # OASM Assistant Application
  oasm-assistant-app:
    container_name: oasm-assistant-app
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - BUILDKIT_INLINE_CACHE=1
      # Multi-arch support

    restart: unless-stopped
    env_file:
      - .env
    ports:
      - "8000:8000" # gRPC port
    environment:
      - POSTGRES_HOST=oasm-assistant-postgresql
      - POSTGRES_PORT=5432
      - SEARXNG_URL=http://oasm-assistant-searxng:8080
      - LLM_BASE_URL=http://oasm-assistant-ollama:11434
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - app-logs:/app/logs # Use named volume instead of bind mount to avoid permission issues
      - ./data:/app/data
      - ./knowledge:/app/knowledge
      - ./app:/app/app
      - ./agents:/app/agents
      - ./common:/app/common
      - ./llms:/app/llms
      - ./tools:/app/tools
    depends_on:
      oasm-assistant-postgresql:
        condition: service_healthy
      oasm-assistant-searxng:
        condition: service_started
    networks:
      - oasm-network
    # Distroless has no curl, health check should be done by gRPC health check
    # or external monitoring tool
    # healthcheck:
    #   test: ["CMD", "grpc_health_probe", "-addr=:8000"]
    #   interval: 30s
    #   timeout: 10s
    #   retries: 3
    #   start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # SearXNG - Meta search engine
  oasm-assistant-searxng:
    container_name: oasm-assistant-searxng
    image: docker.io/searxng/searxng:latest
    restart: unless-stopped
    ports:
      - "127.0.0.1:8080:8080"
    volumes:
      - ./searxng:/etc/searxng:rw
      - searxng-data:/var/cache/searxng:rw
    environment:
      - SEARXNG_BASE_URL=https://localhost:8080/
      - SEARXNG_SECRET=${SEARXNG_SECRET:-changeme}
    networks:
      - oasm-network
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"

  # Ollama - LLM Runtime
  oasm-assistant-ollama:
    container_name: oasm-assistant-ollama
    image: ollama/ollama:latest
    restart: unless-stopped
    ports:
      - "8005:11434"
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - oasm-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
      - OLLAMA_KV_CACHE_TYPE=q8_0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # vLLM - High-performance LLM Inference Server
  oasm-assistant-vllm:
    container_name: oasm-assistant-vllm
    image: vllm/vllm-openai:latest
    restart: unless-stopped
    ports:
      - "8006:8000"
    volumes:
      - vllm-data:/root/.cache/huggingface
    networks:
      - oasm-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      # Model configuration - change this to your preferred model
      - VLLM_MODEL=${VLLM_MODEL:-Qwen/Qwen2.5-7B-Instruct}
      # Performance tuning
      - VLLM_GPU_MEMORY_UTILIZATION=${VLLM_GPU_MEMORY_UTILIZATION:-0.9}
      - VLLM_MAX_MODEL_LEN=${VLLM_MAX_MODEL_LEN:-8192}
      - VLLM_TENSOR_PARALLEL_SIZE=${VLLM_TENSOR_PARALLEL_SIZE:-1}
    command: >
      --model ${VLLM_MODEL}
      --host 0.0.0.0
      --port 8000
      --gpu-memory-utilization ${VLLM_GPU_MEMORY_UTILIZATION}
      --max-model-len ${VLLM_MAX_MODEL_LEN}
      --tensor-parallel-size ${VLLM_TENSOR_PARALLEL_SIZE}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # SGLang - Fast and Expressive LLM Inference Server
  oasm-assistant-sglang:
    container_name: oasm-assistant-sglang
    image: lmsysorg/sglang:latest
    restart: unless-stopped
    ports:
      - "8007:30000"
    volumes:
      - sglang-data:/root/.cache/huggingface
    networks:
      - oasm-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      # Model configuration
      - SGLANG_MODEL=${SGLANG_MODEL:-Qwen/Qwen2.5-7B-Instruct}
      # Performance tuning
      - SGLANG_MEM_FRACTION=${SGLANG_MEM_FRACTION:-0.9}
      - SGLANG_CONTEXT_LENGTH=${SGLANG_CONTEXT_LENGTH:-8192}
      - SGLANG_TP_SIZE=${SGLANG_TP_SIZE:-1}
    command: >
      python3 -m sglang.launch_server
      --model-path ${SGLANG_MODEL}
      --host 0.0.0.0
      --port 30000
      --mem-fraction-static ${SGLANG_MEM_FRACTION}
      --context-length ${SGLANG_CONTEXT_LENGTH}
      --tp-size ${SGLANG_TP_SIZE}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:30000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

networks:
  oasm-network:
    driver: bridge
    name: oasm-network

volumes:
  postgres_data:
    name: oasm-assistant-postgres-data
  searxng-data:
    name: oasm-assistant-searxng-data
  ollama-data:
    name: oasm-assistant-ollama-data
  vllm-data:
    name: oasm-assistant-vllm-data
  sglang-data:
    name: oasm-assistant-sglang-data
  app-logs:
    name: oasm-assistant-app-logs
